2024.12.2
1.创建工程，开始“拾光”服务端的开发。
2.根据类图，初步完成类的构建。

2024.12.3
1.熟悉了头文件包含问题，全局变量声明定义问题。
2.开始线程池的搭建...
3.构建了一个Task类，通过函数指针使其存储了一个任意方法函数，及其可变参数。

2024.12.4
1.参数可变的函数的传递与储存：
    一.
        1)void(*callback)(int count, ...);  //函数指针
        2)void A(void(*callback)(int count, ...), int count, ...);  //传递
        3)利用<stdarg.h>的函数获取参数;
        缺点：没有类型及边界检查，不安全，不建议使用;
             要传递的函数写法固定为 类型 函数名(int count, ...)。
    二.
        1)template<typename... Args>
          void(*callback)(Args...);  //错误写法
        2)template<typename... Args>
          void A(void(*callback)(Args...), Args... args);
        3)利用模板参数包和函数参数包实现;
        缺点：不能声明模板函数指针(成员)变量，只能在模板函数中作为参数传递，故不能储存一个可变函数。
    三.
        1)void(*callback)(void*a,void*b,void*c);
        2)void A(void(*callback)(void*a,void*b,void*c),
                 void*a=nullptr,void*b=nullptr,void*c=nullptr);
        3）利用void*实现;
        缺点：参数最大数量固定;
             要传递的函数写法固定为 类型 函数名(void*a,void*b,void*c)。
2024.12.6
1.解决线程池的一些bug。

2024.12.7
1.完善线程池的搭建。
2.写加锁，如果只加一边相当于没加，猜测：锁不住正在读写的线程。
  总结：加锁+加锁=加锁；加锁+不加锁=不加锁。
  例如：写加锁+写加锁=两者加锁状态；
       写加锁+读不加锁=两者不加锁状态，对写没影响，对读有影响,如：连续读几个变量，数据更改不同步。
3.问题：线程池析构函数，不能加锁(会相互阻塞)导致有极小的概率会造成同时join和detach异常终止程序，情况如下：
  线程池正在销毁，突然加了个任务，子线程被唤醒又检测到需要自杀，该线程就有极低概率同时被读写，然后会都执行detach和join，异常终止。
  虽然概率小，但是建议销毁线程池前，等任务执行完，线程自动销毁完。
  
2024.12.8
1.完善上述问题。
2.shared_mutex读写锁(C++17)在条件变量condition_variable(C++11)中不支持，如：wait(unique<mutex>&);
  故：读写锁在这里优势也不大，采用互斥锁。
3.建议多线程环境，读写都加锁。
4.线程池完成测试。

2024.12.9
1.开始实现服务器的网络编程。

2024.12.10
1.epoll，卡在了IO多路复用。

2024.12.11
1.epoll阶段基本结束。

2024.12.13
1.发现将主线程的变量通过指针传递到子线程的隐患，改写添加任务函数。
2.框架搭建完毕，根据客户端请求添加任务即可。
             
2024.12.14
1.开始数据库阶段的编程。
2.再次调整任务函数，最终以指针传递堆对象来实现。
3.回调函数不能是成员函数。
             
2024.12.15
1.又要大改，使用bind()函数，函数对象，泛型编程技术实现真正的可变参数函数。
2.模板函数声明和定义不能分开写。
3.完成大改。

2024.12.16
1.解决结构体传输问题。
2.明天测试粘包问题，连续发送消息问题。

2024.12.17
1.解决粘包问题。

2024.12.18
1.重写send,recv函数。
2.心跳包代码编写。

2024.12.19
1.开始编写文件传输的代码。

2024.12.20
1.继续编写文件传输代码。

2024.12.22
1.实现大文件传输。

2025.1.6
1.开始重构，设计更好的高并发服务器。

2025.1.12
1.重构部分完成并且运行成功。

2025.1.13
1.EpollOperator重构完成。

2025.1.15
1.消息/文件传输重构完成，加入json序列。

2025.1.16
1.分析心跳包处理：
    1）独立的线程监控，超时没发送心跳包的客户端认定为断开连接，删除数据；
    2）有客户端断开连接需删除数据。
  监控数据属于共享资源，需要加锁或原子对象处理。
2.开始重构UserStatusEvaluator。

2025.1.18
1.UserStatusEvaluator重构完成，并完成测试。

2025.1.19
1.开始进行数据处理。

2025.1.22
1.对注册功能进行测试。
2.UserStatusEvaluator修复bug。
3.UserStatusEvaluator内部加锁实现的，可以考虑使用原子类。

2025.1.23
1.连接断开判断：缓冲区没数据且已断开连接，recv返回0。
2.销毁对象只是该块内存标记为可用，原先的数据依然存在。
  声明char数组时，记得使用memset清零，不然可能自带数据。
3.注册功能测试完成。
4.头像文件发送功能测试完成。

2025.1.24
1.发现重大bug，对fd缓冲区的读不是线程安全的(写入是线程安全的)。
  解决方案(目前采取2)：
  1）使用fd读写前，需要判断fd的状态，故需要增加类似map的容器进行管理记录fd状态。
    缺陷：需要一个容器管理记录，并发环境还需要考虑线程安全问题。
  2）epoll事件返回后，移除对该fd的监视，读取完毕后再重新加入，这样不会出现多线程争夺读取缓冲区的情况。
    缺陷：反复系统调用，开销更大。
2.文件的写入是线程安全的。

2025.1.25
1.当前服务器能力分析：
  1）能同时处理普通消息和文件消息。
  2）能应对客户端各种情况：
      1.连续请求；
      2.连续请求+秒断开；
      3.请求未完成，如文件发了一半，就秒断开。
  ps：只要有客户端断开就会出现epoll_ctl_del error，只是不一定在同一个线程。
2.服务器的架构基本成型，额外添加功能模块即可。

2025.1.26
1.获取文件功能测试完成。

2025.2.1
1.过完年，开工。
  
2025.2.2
1.缓存机制：
  优：减少对数据库的操作。
  劣：占用更多内存。
  ps：目前不采用缓存机制，把它放在优化计划中。
2.功能分析：
  1)登陆、注册     InitControl
  2)请求文件       SendFileControl
  3)发送文件       ReceiveFileControl
  4)请求信息       SendInfoControl
  5)修改信息       ModifyInfoControl
  ps:请求信息 评论、点赞数及是否点赞、关注数及所有id、粉丝数及所有id、点播列表、直播列表
     修改信息 评论、点赞、取消点赞、关注、取消关注、删除我的视频
3.数据库分析
  1)用户    User       id pw nikeName pictureUrl
  2)视频    Video      id publisherId videoUrl profile previewUrl
  3)评论    Comment    publisherId videoId content time
  4)视频点赞 VideoLike  videoId userId
  5)关注    Attention  userId followerId
  
2025.2.3
1.修改信息、请求信息测试完成，如评论。
2.目前所有功能模块都有测试，后续依葫芦画瓢完善功能即可。

2025.2.4
1.已实现的功能：
  1)登陆和注册；  //已实现
  2)心跳包处理；  //已实现
  3)发送文件：头像文件、点播视频文件；  //已实现
  4)请求文件：头像文件、点播视频文件；  //已实现
  5)修改信息：增加一条某个视频的评论、删除我的视频、点赞、取消点赞、关注、取消关注、开启直播、结束直播；  //已实现
  6)请求信息：获取某个视频的所有评论、获取点播列表信息、获取点赞数及是否点赞、获取关注的所有id、获取粉丝所有id，获取直播列表信息。  //已实现
  
2025.2.8
1.信息类型作用说明:
  1)Comment   评论+获取评论
  2)Like      点赞+获取点赞信息
  3)Follow    关注+获取关注信息
  4)Fans      获取粉丝信息
  5)VodList   获取点播列表信息
  6)LiveList  开启直播+获取直播列表信息
  7)Video     删除我的视频
2.客户端没有正常结束直播并断开，而导致直播结束并没有告诉服务器。
  解决：每次断开时处理相应直播信息。
3.目前整理的数据服务功能实现完毕。

2025.2.18
1.优化/修复bug:
  1)当前主线程接收新连接——>多线程处理。ps:最后还是改回以前版本，epoll水平模式，主线程接收，但是socket调整为非阻塞模式。
  2)创建数据库Server。
  
2025.2.20
1.优化/修复bug:
  1)处理新连接：epoll水平模式，主线程接收——>epoll水平模式，主线程循环非阻塞accept。
  2)错误码部分处理，不会一直出现红色的错误提示，如epoll_ctl_del error、recv error。
  
2025.2.21(这天是Boost没学明白，胡言乱语...)
1.Boost的asio库的使用：
  1)服务端的循环监听依赖IO多路复用，在使用IO多路复用的基础上，就没必要使用异步操作。
  误区：一开始以为使用Boost的asio库就一定要用异步操作，实则不然，目前并没有发现其用武之地，但是该库封装了很多优秀的网络api，
       比原始的要方便快捷。
2.计划保留C库版本的网络通信，即开闭原则，聚合原有Server类，即合成复用原则。(后来发现直接构造新BoostServer，不需要复用)

2025.2.22
1.异步操作可以跟递归搭配，绝绝子，如下：
  1)异步接收新连接，回调函数中递归，解决了循环处理新连接的问题。
  2)在上述回调函数中，异步读取数据，又在其回调函数中递归自己，解决了循环处理新信息的问题。
  ps:这样就替代了原版本epoll的功能。
  
2025.2.24
1.优化/修复bug:
  1)重构单例的实现方式。
  2)线程池也设计为单例。
  
2025.2.25
1.开始实现BoostServer。
2.boost::asio::ip::tcp::socket不能被拷贝赋值，只能移动传递。

2025.2.26
1.BoostServer的设计：
  1)一个io_context，多线程run()，以此来实现多线程asio;
  2)异步递归接收新连接;
  3)异步递归读取数据。
  ps:异步的作用除了监听事件的发生，还有就绪后其回调函数进入一个事件队列被空闲子线程取出执行。
  问题：在事件频繁的条件下，可能来不及接收新连接，即客户端连接会失败!
  解决方案：可以采用实现多线程asio的另一种方法，每个子线程拥有自己的io_context，
          主线程同步递归接收新连接，按最优路径传递socket到子线程。
  目前：只是前者代码编写很简单，而后者相当于重构一个线程池，故先写一个前者的版本OvO。
  
2025.2.27
1.完成部分BoostServer的代码编写:
  1)测试完Register；
  2)测试完Login；
  3)测试完Heart。
  
2025.2.28
1.继续BoostServer的代码编写:
  1)测试完SendFile;
  2)测试完GetFile;
  3)测试完GetInfo;
  4)测试完ModifyInfo;
2.Boost的receive()并不是阻塞读取完数据，目前感觉跟read_some差不多，不过还可以用boost:asio::read()。
3.BoostServer实现完毕。


待完成：

优化计划：
1.服务器的缓存机制：实体类及其broker类的编写、（虚拟）代理模式。
2.图像文件格式处理。
3.视频文件格式处理。

             
             
             
             

             
             
             
             
             
             
             
             
             
             
             
             
             
             

             
             
             
             
             
             
